Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{cocoeval,
title = {{COCO Key-Point Evaluation}},
url = {http://cocodataset.org/{\#}keypoints-eval},
urldate = {19/05/2020}
}
@article{Arguin2004,
abstract = {A visual search experiment using synthetic three-dimensional objects is reported. The target shared its constituent parts, the spatial organization of its parts, or both with the distractors displayed with it. Sharing of parts and sharing of spatial organization both negatively affected visual search performance, and these effects were strictly additive. These findings support theories of complex visual object perception that assume a parsing of the stimulus into its higher-order constituents (volumetric parts or visible surfaces). The additivity of the effects demonstrates that information on parts and information on spatial organization are processed independently in visual search.},
author = {Arguin, Martin and Saumier, Daniel},
doi = {10.1111/j.0956-7976.2004.00731.x},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/Independent Processing of Parts and of Their Spatial Organization in Complex Visual Objects.pdf:pdf},
issn = {09567976},
journal = {Psychological Science},
number = {9},
pages = {629--633},
title = {{Independent processing of parts and of their spatial organization in complex visual objects}},
volume = {15},
year = {2004}
}
@article{Grabner2018,
abstract = {We propose a scalable, efficient and accurate approach to retrieve 3D models for objects in the wild. Our contribution is twofold. We first present a 3D pose estimation approach for object categories which significantly outperforms the state-of-the-art on Pascal3D+. Second, we use the estimated pose as a prior to retrieve 3D models which accurately represent the geometry of objects in RGB images. For this purpose, we render depth images from 3D models under our predicted pose and match learned image descriptors of RGB images against those of rendered depth images using a CNN-based multi-view metric learning approach. In this way, we are the first to report quantitative results for 3D model retrieval on Pascal3D+, where our method chooses the same models as human annotators for 50{\%} of the validation images on average. In addition, we show that our method, which was trained purely on Pascal3D+, retrieves rich and accurate 3D models from ShapeNet given RGB images of objects in the wild.},
archivePrefix = {arXiv},
arxivId = {1803.11493},
author = {Grabner, Alexander and Roth, Peter M. and Lepetit, Vincent},
doi = {10.1109/CVPR.2018.00319},
eprint = {1803.11493},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/3D Pose Estimation and 3D Model Retrieval for Objects in the Wild.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {3022--3031},
title = {{3D Pose Estimation and 3D Model Retrieval for Objects in the Wild}},
year = {2018}
}
@article{Biederman1987,
abstract = {The perceptual recognition of objects is conceptualized to be a process in which the image of the input is segmented at regions of deep concavity into an arrangement of simple geometric components, such as blocks, cylinders, wedges, and cones. The fundamental assumption of the proposed theory, recognition-by-components (RBC), is that a modest set of generalized-cone components, called geons (N ≤ 36), can be derived from contrasts of five readily detectable properties of edges in a two-dimensional image: curvature, collinearity, symmetry, parallelism, and cotermination. The detection of these properties is generally invariant over viewing position and image quality and consequently allows robust object perception when the image is projected from a novel viewpoint or is degraded. RBC thus provides a principled account of the heretofore undecided relation between the classic principles of perceptual organization and pattern recognition: The constraints toward regularization (Pragnanz) characterize not the complete object but the object's components. Representational power derives from an allowance of free combinations of the geons. A Principle of Componential Recovery can account for the major phenomena of object recognition: If an arrangement of two or three geons can be recovered from the input, objects can be quickly recognized even when they are occluded, novel, rotated in depth, or extensively degraded. The results from experiments on the perception of briefly presented pictures by human observers provide empirical support for the theory. {\textcopyright} 1987 American Psychological Association.},
author = {Biederman, Irving},
doi = {10.1037/0033-295X.94.2.115},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/Recognition-by-Components A Theory of Human Image Understanding Irving Biederman.pdf:pdf},
issn = {0033295X},
journal = {Psychological Review},
number = {2},
pages = {115--147},
pmid = {3575582},
title = {{Recognition-by-Components: A Theory of Human Image Understanding}},
volume = {94},
year = {1987}
}
@article{Xiang2018,
abstract = {Estimating the 6D pose of known objects is important for robots to interact with the real world. The problem is challenging due to the variety of objects as well as the complexity of a scene caused by clutter and occlusions between objects. In this work, we introduce PoseCNN, a new Convolutional Neural Network for 6D object pose estimation. PoseCNN estimates the 3D translation of an object by localizing its center in the image and predicting its distance from the camera. The 3D rotation of the object is estimated by regressing to a quaternion representation. We also introduce a novel loss function that enables PoseCNN to handle symmetric objects. In addition, we contribute a large scale video dataset for 6D object pose estimation named the YCB-Video dataset. Our dataset provides accurate 6D poses of 21 objects from the YCB dataset observed in 92 videos with 133,827 frames. We conduct extensive experiments on our YCB-Video dataset and the OccludedLINEMOD dataset to show that PoseCNN is highly robust to occlusions, can handle symmetric objects, and provide accurate pose estimation using only color images as input. When using depth data to further refine the poses, our approach achieves state-of-the-art results on the challenging OccludedLINEMOD dataset. Our code and dataset are available at https://rse-lab.cs.washington.edu/projects/posecnn/.},
archivePrefix = {arXiv},
arxivId = {1711.00199},
author = {Xiang, Yu and Schmidt, Tanner and Narayanan, Venkatraman and Fox, Dieter},
doi = {10.15607/rss.2018.xiv.019},
eprint = {1711.00199},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/PoseCNN A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes.pdf:pdf},
title = {{PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes}},
year = {2018}
}
@article{Moreno-noguer2007,
author = {Moreno-noguer, Francesc},
doi = {10.1007/s10878-015-9827-4},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/EPnP Accurate Non-Iterative O(n) Solution to the PnP Problem.pdf:pdf},
isbn = {4121693752},
issn = {1382-6905},
journal = {Iccv},
keywords = {absolute orientation,perspective- n -point,pose estimation},
title = {{EPnP Accurate Non-Iterative O ( n ) Solution to the P n P Problem ∗}},
year = {2007}
}
@article{Chen2018,
abstract = {Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89{\%} and 82.1{\%} without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at https://github.com/tensorflow/models/tree/master/research/deeplab.},
archivePrefix = {arXiv},
arxivId = {1802.02611},
author = {Chen, Liang Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
doi = {10.1007/978-3-030-01234-2_49},
eprint = {1802.02611},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation - DeepLabV3.pdf:pdf},
isbn = {9783030012335},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Depthwise separable convolution,Encoder-decoder,Semantic image segmentation,Spatial pyramid pooling},
pages = {833--851},
title = {{Encoder-decoder with atrous separable convolution for semantic image segmentation}},
volume = {11211 LNCS},
year = {2018}
}
@article{Erhan2014,
abstract = {Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a net- work that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number ofoutputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood ofcontaining any object of interest. The model naturally handles a variable number of instances for each class and allows for cross- class generalization at the highest levels ofthe network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.},
author = {Erhan, Dumitru and Szegedy, Christian and Toshev, Alexander and Anguelov, Dragomir},
doi = {10.1109/ICCONS.2017.8250570},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/MultiBox.pdf:pdf},
isbn = {9781538627457},
journal = {Cvpr},
keywords = {Neural network,Object detection},
title = {{Scalable Object Detection using Deep Neural Networks Dumitru}},
year = {2014}
}
@article{He2017,
abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.},
archivePrefix = {arXiv},
arxivId = {1703.06870},
author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
doi = {10.1109/ICCV.2017.322},
eprint = {1703.06870},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/Mask R-CNN.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2980--2988},
pmid = {29994331},
title = {{Mask R-CNN}},
volume = {2017-Octob},
year = {2017}
}
@article{Everingham2014,
abstract = {The Pascal Visual Object Classes (VOC) challenge consists of two components: (i) a publicly available dataset of images together with ground truth annotation and standardised evaluation software; and (ii) an annual competition and workshop. There are five challenges: classification, detection, segmentation, action classification, and person layout. In this paper we provide a review of the challenge from 2008–2012. The paper is intended for two audiences: algorithm designers, researchers who want to see what the state of the art is, as measured by performance on the VOC datasets, along with the limitations and weak points of the current generation of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the process and our recommendations for the organisation of future challenges. To analyse the performance of submitted algorithms on the VOC datasets we introduce a number of novel evaluation methods: a bootstrapping method for determining whether differences in the performance of two algorithms are significant or not; a normalised average precision so that performance can be compared across classes with different proportions of positive instances; a clustering method for visualising the performance across multiple algorithms so that the hard and easy images can be identified; and the use of a joint classifier over the submitted algorithms in order to measure their complementarity and combined performance. We also analyse the community's progress through time using the methods of Hoiem et al. (Proceedings of European Conference on Computer Vision, 2012) to identify the types of occurring errors. We conclude the paper with an appraisal of the aspects of the challenge that worked well, and those that could be improved in future challenges.},
author = {Everingham, Mark and Eslami, S. M.Ali and {Van Gool}, Luc and Williams, Christopher K.I. and Winn, John and Zisserman, Andrew},
doi = {10.1007/s11263-014-0733-5},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/The PASCAL Visual Object Classes Challenge A Retrospective.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Database,Object detection,Object recognition,Segmentation},
number = {1},
pages = {98--136},
title = {{The Pascal Visual Object Classes Challenge: A Retrospective}},
volume = {111},
year = {2014}
}
@article{Xiang2015,
abstract = {Despite the great progress achieved in recognizing objects as 2D bounding boxes in images, it is still very challenging to detect occluded objects and estimate the 3D properties of multiple objects from a single image. In this paper, we propose a novel object representation, 3D Voxel Pattern (3DVP), that jointly encodes the key properties of objects including appearance, 3D shape, viewpoint, occlusion and truncation. We discover 3DVPs in a data-driven way, and train a bank of specialized detectors for a dictionary of 3DVPs. The 3DVP detectors are capable of detecting objects with specific visibility patterns and transferring the meta-data from the 3DVPs to the detected objects, such as 2D segmentation mask, 3D pose as well as occlusion or truncation boundaries. The transferred meta-data allows us to infer the occlusion relationship among objects, which in turn provides improved object recognition results. Experiments are conducted on the KITTI detection benchmark [17] and the outdoor-scene dataset [41]. We improve state-of-the-art results on car detection and pose estimation with notable margins (6{\%} in difficult data of KITTI). We also verify the ability of our method in accurately segmenting objects from the background and localizing them in 3D.},
author = {Xiang, Yu and Choi, Wongun and Lin, Yuanqing and Savarese, Silvio},
doi = {10.1109/CVPR.2015.7298800},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/Data-Driven 3D Voxel Patterns for Object Category Recognition.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1903--1911},
title = {{Data-driven 3D Voxel Patterns for object category recognition}},
volume = {07-12-June},
year = {2015}
}
@article{Zhao2019,
abstract = {Due to object detection's close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles that combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy, and optimization function. In this paper, we provide a review of deep learning-based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely, the convolutional neural network. Then, we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection, and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network-based learning systems.},
archivePrefix = {arXiv},
arxivId = {1807.05511},
author = {Zhao, Zhong Qiu and Zheng, Peng and Xu, Shou Tao and Wu, Xindong},
doi = {10.1109/TNNLS.2018.2876865},
eprint = {1807.05511},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/ODoverview.pdf:pdf},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Deep learning,neural network,object detection},
number = {11},
pages = {3212--3232},
title = {{Object Detection with Deep Learning: A Review}},
volume = {30},
year = {2019}
}
@article{Chen2019,
abstract = {We present an unsupervised learning approach to recover 3D human pose from 2D skeletal joints extracted from a single image. Our method does not require any multi-view image data, 3D skeletons, correspondences between 2D-3D points, or use previously learned 3D priors during training. A lifting network accepts 2D landmarks as inputs and generates a corresponding 3D skeleton estimate. During training, the recovered 3D skeleton is reprojected on random camera viewpoints to generate new "synthetic" 2D poses. By lifting the synthetic 2D poses back to 3D and re-projecting them in the original camera view, we can define self-consistency loss both in 3D and in 2D. The training can thus be self supervised by exploiting the geometric self-consistency of the lift-reproject-lift process. We show that self-consistency alone is not sufficient to generate realistic skeletons, however adding a 2D pose discriminator enables the lifter to output valid 3D poses. Additionally, to learn from 2D poses "in the wild", we train an unsupervised 2D domain adapter network to allow for an expansion of 2D data. This improves results and demonstrates the usefulness of 2D pose data for unsupervised 3D lifting. Results on Human3.6M dataset for 3D human pose estimation demonstrate that our approach improves upon the previous unsupervised methods by 30{\%} and outperforms many weakly supervised approaches that explicitly use 3D data.},
archivePrefix = {arXiv},
arxivId = {1904.04812},
author = {Chen, Ching-Hang and Tyagi, Ambrish and Agrawal, Amit and Drover, Dylan and MV, Rohith and Stojanov, Stefan and Rehg, James M.},
eprint = {1904.04812},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/Unsupervised 3D Pose Estimation with Geometric Self-Supervision.pdf:pdf},
title = {{Unsupervised 3D Pose Estimation with Geometric Self-Supervision}},
url = {http://arxiv.org/abs/1904.04812},
year = {2019}
}
@article{Jorgensen2019,
abstract = {Three-dimensional object detection from a single view is a challenging task which, if performed with good accuracy, is an important enabler of low-cost mobile robot perception. Previous approaches to this problem suffer either from an overly complex inference engine or from an insufficient detection accuracy. To deal with these issues, we present SS3D, a single-stage monocular 3D object detector. The framework consists of (i) a CNN, which outputs a redundant representation of each relevant object in the image with corresponding uncertainty estimates, and (ii) a 3D bounding box optimizer. We show how modeling heteroscedastic uncertainty improves performance upon our baseline, and furthermore, how back-propagation can be done through the optimizer in order to train the pipeline end-to-end for additional accuracy. Our method achieves SOTA accuracy on monocular 3D object detection, while running at 20 fps in a straightforward implementation. We argue that the SS3D architecture provides a solid framework upon which high performing detection systems can be built, with autonomous driving being the main application in mind.},
archivePrefix = {arXiv},
arxivId = {1906.08070},
author = {J{\"{o}}rgensen, Eskil and Zach, Christopher and Kahl, Fredrik},
eprint = {1906.08070},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/SS3D.pdf:pdf},
pages = {1--10},
title = {{Monocular 3D Object Detection and Box Fitting Trained End-to-End Using Intersection-over-Union Loss}},
url = {http://arxiv.org/abs/1906.08070},
year = {2019}
}
@article{Chen2017,
abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
archivePrefix = {arXiv},
arxivId = {1706.05587},
author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
eprint = {1706.05587},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/DeepLabV3.pdf:pdf},
title = {{Rethinking Atrous Convolution for Semantic Image Segmentation}},
url = {http://arxiv.org/abs/1706.05587},
year = {2017}
}
@article{Felzenszwalb2009,
abstract = {We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL datasets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin- sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI-SVM in terms of latent variables. A latent SVM is semi-convex and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function.},
author = {Felzenszwalb, Pedro and Girshick, Ross and McAllester, David and Ramanan, Deva},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/DPM2d.pdf:pdf},
isbn = {0162-8828 VO - 32},
issn = {00189162},
keywords = {image detection,machine intelligence,object visualization,patterns},
pmid = {20634557},
title = {{Object Detection with Discriminatively Trained Part Based Models}},
year = {2009}
}
@article{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model. {\textcopyright} 2014 Springer International Publishing.},
archivePrefix = {arXiv},
arxivId = {1405.0312},
author = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
doi = {10.1007/978-3-319-10602-1_48},
eprint = {1405.0312},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/COCO.pdf:pdf},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 5},
pages = {740--755},
title = {{Microsoft COCO: Common objects in context}},
volume = {8693 LNCS},
year = {2014}
}
@article{Brazil2019,
abstract = {Understanding the world in 3D is a critical component of urban autonomous driving. Generally, the combination of expensive LiDAR sensors and stereo RGB imaging has been paramount for successful 3D object detection algorithms, whereas monocular image-only methods experience drastically reduced performance. We propose to reduce the gap by reformulating the monocular 3D detection problem as a standalone 3D region proposal network. We leverage the geometric relationship of 2D and 3D perspectives, allowing 3D boxes to utilize well-known and powerful convolutional features generated in the image-space. To help address the strenuous 3D parameter estimations, we further design depth-aware convolutional layers which enable location specific feature development and in consequence improved 3D scene understanding. Compared to prior work in monocular 3D detection, our method consists of only the proposed 3D region proposal network rather than relying on external networks, data, or multiple stages. M3D-RPN is able to significantly improve the performance of both monocular 3D Object Detection and Bird's Eye View tasks within the KITTI urban autonomous driving dataset, while efficiently using a shared multi-class model.},
archivePrefix = {arXiv},
arxivId = {1907.06038},
author = {Brazil, Garrick and Liu, Xiaoming},
doi = {10.1109/ICCV.2019.00938},
eprint = {1907.06038},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/M3D-RPN.pdf:pdf},
isbn = {9781728148038},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {9286--9295},
title = {{M3D-RPN: Monocular 3D region proposal network for object detection}},
volume = {2019-October},
year = {2019}
}
@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.1556v6},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {arXiv:1409.1556v6},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/VGG.pdf:pdf},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
pages = {1--14},
title = {{Very deep convolutional networks for large-scale image recognition}},
year = {2015}
}
@article{Najibi2016,
abstract = {We introduce G-CNN, an object detection technique based on CNNs which works without proposal algorithms. G-CNN starts with a multi-scale grid of fixed bounding boxes. We train a regressor to move and scale elements of the grid towards objects iteratively. G-CNN models the problem of object detection as finding a path from a fixed grid to boxes tightly surrounding the objects. G-CNN with around 180 boxes in a multi-scale grid performs comparably to Fast R-CNN which uses around 2K bounding boxes generated with a proposal technique. This strategy makes detection faster by removing the object proposal stage as well as reducing the number of boxes to be processed.},
archivePrefix = {arXiv},
arxivId = {1512.07729},
author = {Najibi, Mahyar and Rastegari, Mohammad and Davis, Larry S.},
doi = {10.1109/CVPR.2016.260},
eprint = {1512.07729},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/GCNN.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {2369--2377},
title = {{G-CNN: An Iterative Grid Based Object Detector}},
volume = {2016-Decem},
year = {2016}
}
@article{HosseiniJafari2019,
abstract = {We address the task of 6D pose estimation of known rigid objects from single input images in scenarios where the objects are partly occluded. Recent RGB-D-based methods are robust to moderate degrees of occlusion. For RGB inputs, no previous method works well for partly occluded objects. Our main contribution is to present the first deep learning-based system that estimates accurate poses for partly occluded objects from RGB-D and RGB input. We achieve this with a new instance-aware pipeline that decomposes 6D object pose estimation into a sequence of simpler steps, where each step removes specific aspects of the problem. The first step localizes all known objects in the image using an instance segmentation network, and hence eliminates surrounding clutter and occluders. The second step densely maps pixels to 3D object surface positions, so called object coordinates, using an encoder-decoder network, and hence eliminates object appearance. The third, and final, step predicts the 6D pose using geometric optimization. We demonstrate that we significantly outperform the state-of-the-art for pose estimation of partly occluded objects for both RGB and RGB-D input.},
archivePrefix = {arXiv},
arxivId = {1712.01924},
author = {{Hosseini Jafari}, Omid and Mustikovela, Siva Karthik and Pertsch, Karl and Brachmann, Eric and Rother, Carsten},
doi = {10.1007/978-3-030-20893-6_30},
eprint = {1712.01924},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/iPose Instance-Aware 6D Pose Estimation of Partly Occluded Objects.pdf:pdf},
isbn = {9783030208929},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {477--492},
title = {{iPose: Instance-Aware 6D Pose Estimation of Partly Occluded Objects}},
volume = {11363 LNCS},
year = {2019}
}
@article{Xiang2014,
abstract = {3D object detection and pose estimation methods have become popular in recent years since they can handle ambiguities in 2D images and also provide a richer description for objects compared to 2D object detectors. However, most of the datasets for 3D recognition are limited to a small amount of images per category or are captured in controlled environments. In this paper, we contribute PASCAL3D+ dataset, which is a novel and challenging dataset for 3D object detection and pose estimation. PASCAL3D+ augments 12 rigid categories of the PASCAL VOC 2012 [4] with 3D annotations. Furthermore, more images are added for each category from ImageNet [3]. PASCAL3D+ images exhibit much more variability compared to the existing 3D datasets, and on average there are more than 3,000 object instances per category. We believe this dataset will provide a rich testbed to study 3D detection and pose estimation and will help to significantly push forward research in this area. We provide the results of variations of DPM [6] on our new dataset for object detection and viewpoint estimation in different scenarios, which can be used as baselines for the community. Our benchmark is available online at http://cvgl.stanford.edu/ projects/pascal3d {\textcopyright} 2014 IEEE.},
author = {Xiang, Yu and Mottaghi, Roozbeh and Savarese, Silvio},
doi = {10.1109/WACV.2014.6836101},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/Beyond PASCAL A Benchmark for 3D Object Detection in the Wild.pdf:pdf},
isbn = {9781479949854},
journal = {2014 IEEE Winter Conference on Applications of Computer Vision, WACV 2014},
pages = {75--82},
publisher = {IEEE},
title = {{Beyond PASCAL: A benchmark for 3D object detection in the wild}},
year = {2014}
}
@article{Redmon2016,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02640v5},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
doi = {10.1109/CVPR.2016.91},
eprint = {arXiv:1506.02640v5},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/YOLO.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {779--788},
title = {{You only look once: Unified, real-time object detection}},
volume = {2016-Decem},
year = {2016}
}
@article{Oberweger2018,
abstract = {We introduce a novel method for robust and accurate 3D object pose estimation from a single color image under large occlusions. Following recent approaches, we first predict the 2D projections of 3D points related to the target object and then compute the 3D pose from these correspondences using a geometric method. Unfortunately, as the results of our experiments show, predicting these 2D projections using a regular CNN or a Convolutional Pose Machine is highly sensitive to partial occlusions, even when these methods are trained with partially occluded examples. Our solution is to predict heatmaps from multiple small patches independently and to accumulate the results to obtain accurate and robust predictions. Training subsequently becomes challenging because patches with similar appearances but different positions on the object correspond to different heatmaps. However, we provide a simple yet effective solution to deal with such ambiguities. We show that our approach outperforms existing methods on two challenging datasets: The Occluded LineMOD dataset and the YCB-Video dataset, both exhibiting cluttered scenes with highly occluded objects.},
archivePrefix = {arXiv},
arxivId = {1804.03959},
author = {Oberweger, Markus and Rad, Mahdi and Lepetit, Vincent},
doi = {10.1007/978-3-030-01267-0_8},
eprint = {1804.03959},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/Making Deep Heatmaps Robust to Partial Occlusions for 3D Object Pose Estimation.pdf:pdf},
isbn = {9783030012663},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {3D object pose estimation,Heatmaps,Occlusions},
pages = {125--141},
title = {{Making deep heatmaps robust to partial occlusions for 3D object pose estimation}},
volume = {11219 LNCS},
year = {2018}
}
@article{Deng2009,
author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
doi = {10.14842/jpnjnephrol1959.20.1221},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/ImageNet A Large-Scale Hierarchical Image Database.pdf:pdf},
issn = {03852385},
title = {{ImageNet: A Large-Scale Hierarchical Image Database}},
year = {2009}
}
@article{Fu2017,
abstract = {The main contribution of this paper is an approach for introducing additional context into state-of-the-art general object detection. To achieve this we first combine a state-of-the-art classifier (Residual-101[14]) with a fast detection framework (SSD[18]). We then augment SSD+Residual-101 with deconvolution layers to introduce additional large-scale context in object detection and improve accuracy, especially for small objects, calling our resulting system DSSD for deconvolutional single shot detector. While these two contributions are easily described at a high-level, a naive implementation does not succeed. Instead we show that carefully adding additional stages of learned transformations, specifically a module for feed-forward connections in deconvolution and a new output module, enables this new approach and forms a potential way forward for further detection research. Results are shown on both PASCAL VOC and COCO detection. Our DSSD with {\$}513 \backslashtimes 513{\$} input achieves 81.5{\%} mAP on VOC2007 test, 80.0{\%} mAP on VOC2012 test, and 33.2{\%} mAP on COCO, outperforming a state-of-the-art method R-FCN[3] on each dataset.},
archivePrefix = {arXiv},
arxivId = {1701.06659},
author = {Fu, Cheng-Yang and Liu, Wei and Ranga, Ananth and Tyagi, Ambrish and Berg, Alexander C.},
eprint = {1701.06659},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/DSSD.pdf:pdf},
title = {{DSSD : Deconvolutional Single Shot Detector}},
url = {http://arxiv.org/abs/1701.06659},
year = {2017}
}
@article{Kehl2017,
abstract = {We present a novel method for detecting 3D model instances and estimating their 6D poses from RGB data in a single shot. To this end, we extend the popular SSD paradigm to cover the full 6D pose space and train on synthetic model data only. Our approach competes or surpasses current state-of-the-art methods that leverage RGBD data on multiple challenging datasets. Furthermore, our method produces these results at around 10Hz, which is many times faster than the related methods. For the sake of reproducibility, we make our trained networks and detection code publicly available.},
archivePrefix = {arXiv},
arxivId = {1711.10006},
author = {Kehl, Wadim and Manhardt, Fabian and Tombari, Federico and Ilic, Slobodan and Navab, Nassir},
doi = {10.1109/ICCV.2017.169},
eprint = {1711.10006},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/SSD-6D Making RGB-Based 3D Detection and 6D Pose Estimation Great Again.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {1530--1538},
title = {{SSD-6D: Making RGB-Based 3D Detection and 6D Pose Estimation Great Again}},
volume = {2017-Octob},
year = {2017}
}
@inproceedings{He2016,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn-ing residual functions with reference to the layer inputs, in-stead of learning unreferenced functions. We provide com-prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complex-ity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex-tremely deep representations, we obtain a 28{\%} relative im-provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions 1 , where we also won the 1st places on the tasks of ImageNet detection, ImageNet local-ization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/ResNet.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {1664-1078},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
year = {2016}
}
@article{Rad2017,
abstract = {We introduce a novel method for 3D object detection and pose estimation from color images only. We first use segmentation to detect the objects of interest in 2D even in presence of partial occlusions and cluttered background. By contrast with recent patch-based methods, we rely on a 'holistic' approach: We apply to the detected objects a Convolutional Neural Network (CNN) trained to predict their 3D poses in the form of 2D projections of the corners of their 3D bounding boxes. This, however, is not sufficient for handling objects from the recent T-LESS dataset: These objects exhibit an axis of rotational symmetry, and the similarity of two images of such an object under two different poses makes training the CNN challenging. We solve this problem by restricting the range of poses used for training, and by introducing a classifier to identify the range of a pose at run-time before estimating it. We also use an optional additional step that refines the predicted poses. We improve the state-of-the-art on the LINEMOD dataset from 73.7{\%} [2] to 89.3{\%} of correctly registered RGB frames. We are also the first to report results on the Occlusion dataset [1 ] using color images only. We obtain 54{\%} of frames passing the Pose 6D criterion on average on several sequences of the T-LESS dataset, compared to the 67{\%} of the state-of-the-art [10] on the same sequences which uses both color and depth. The full approach is also scalable, as a single network can be trained for multiple objects simultaneously.},
archivePrefix = {arXiv},
arxivId = {1703.10896},
author = {Rad, Mahdi and Lepetit, Vincent},
doi = {10.1109/ICCV.2017.413},
eprint = {1703.10896},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/BB8 A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without using Depth.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {3848--3856},
title = {{BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth}},
volume = {2017-Octob},
year = {2017}
}
@article{Kar2015,
abstract = {Object reconstruction from a single image - in the wild - is a problem where we can make progress and get meaningful results today. This is the main message of this paper, which introduces an automated pipeline with pixels as inputs and 3D surfaces of various rigid categories as outputs in images of realistic scenes. At the core of our approach are deformable 3D models that can be learned from 2D annotations available in existing object detection datasets, that can be driven by noisy automatic object segmentations and which we complement with a bottom-up module for recovering high-frequency shape details. We perform a comprehensive quantitative analysis and ablation study of our approach using the recently introduced PASCAL 3D+ dataset and show very encouraging automatic reconstructions on PASCAL VOC.},
archivePrefix = {arXiv},
arxivId = {1411.6069},
author = {Kar, Abhishek and Tulsiani, Shubham and Carreira, Jo{\~{a}}o and Malik, Jitendra},
doi = {10.1109/CVPR.2015.7298807},
eprint = {1411.6069},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/Category-Specific Object Reconstruction from a Single Image.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1966--1974},
title = {{Category-specific object reconstruction from a single image}},
volume = {07-12-June},
year = {2015}
}
@article{Girshick2015,
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
archivePrefix = {arXiv},
arxivId = {1504.08083},
author = {Girshick, Ross},
doi = {10.1109/ICCV.2015.169},
eprint = {1504.08083},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/FastRCNN.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {1440--1448},
title = {{Fast R-CNN}},
volume = {2015 Inter},
year = {2015}
}
@article{Bregler2000,
abstract = {This paper addresses the problem of recovering 3D non-rigid shape models from image sequences. For example, given a video recording of a talking person, we would like to estimate a 3D model of the lips and the full face and its internal modes of variation. Many solutions that recover 3D shape from 2D image sequences have been proposed; these so-called structure-from-motion techniques usually assume that the 3D object is rigid. For example, Tomasi and Kanades' factorization technique is based on a rigid shape matrix, which produces a tracking matrix of rank 3 under orthographic projection. We propose a novel technique based on a non-rigid model, where the 3D shape in each frame is a linear combination of a set of basis shapes. Under this model, the tracking matrix is of higher rank, and can be factored in a three-step process to yield pose, configuration and shape. To the best of our knowledge, this is the first model free approach that can recover from single-view video sequences nonrigid shape models. We demonstrate this new algorithm on several video sequences. We were able to recover 3D non-rigid human face and animal models with high accuracy.},
author = {Bregler, Christoph and Hertzmann, Aaron and Biermann, Henning},
doi = {10.1109/CVPR.2000.854941},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/Recovering Non-Rigid 3D Shape from Image Streams.pdf:pdf},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {690--696},
title = {{Recovering non-rigid 3D shape from image streams}},
volume = {2},
year = {2000}
}
@article{Ren2017,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features - using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
archivePrefix = {arXiv},
arxivId = {1506.01497},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1109/TPAMI.2016.2577031},
eprint = {1506.01497},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/FasterRCNN.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Object detection,convolutional neural network,region proposal},
number = {6},
pages = {1137--1149},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
volume = {39},
year = {2017}
}
@article{Pepik2012,
abstract = {Current object class recognition systems typically target 2D bounding box localization, encouraged by benchmark data sets, such as Pascal VOC. While this seems suitable for the detection of individual objects, higher-level applications such as 3D scene understanding or 3D object tracking would benefit from more fine-grained object hypotheses incorporating 3D geometric information, such as viewpoints or the locations of individual parts. In this paper, we help narrowing the representational gap between the ideal input of a scene understanding system and object class detector output, by designing a detector particularly tailored towards 3D geometric reasoning. In particular, we extend the successful discriminatively trained deformable part models to include both estimates of viewpoint and 3D parts that are consistent across viewpoints. We experimentally verify that adding 3D geometric information comes at minimal performance loss w.r.t. 2D bounding box localization, but outperforms prior work in 3D viewpoint estimation and ultra-wide baseline matching. {\textcopyright} 2012 IEEE.},
author = {Pepik, Bojan and Stark, Michael and Gehler, Peter and Schiele, Bernt},
doi = {10.1109/CVPR.2012.6248075},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/Teaching 3D Geometry to Deformable Part Models.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
number = {June},
pages = {3362--3369},
title = {{Teaching 3D geometry to deformable part models}},
year = {2012}
}
@article{Hung2019,
abstract = {Parts provide a good intermediate representation of objects that is robust with respect to the camera, pose and appearance variations. Existing works on part segmentation is dominated by supervised approaches that rely on large amounts of manual annotations and can not generalize to unseen object categories. We propose a self-supervised deep learning approach for part segmentation, where we devise several loss functions that aids in predicting part segments that are geometrically concentrated, robust to object variations and are also semantically consistent across different object instances. Extensive experiments on different types of image collections demonstrate that our approach can produce part segments that adhere to object boundaries and also more semantically consistent across object instances compared to existing self-supervised techniques.},
archivePrefix = {arXiv},
arxivId = {1905.01298},
author = {Hung, Wei-Chih and Jampani, Varun and Liu, Sifei and Molchanov, Pavlo and Yang, Ming-Hsuan and Kautz, Jan},
eprint = {1905.01298},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/SCOPS Self-Supervised Co-Part Segmentation.pdf:pdf},
title = {{SCOPS: Self-Supervised Co-Part Segmentation}},
url = {http://arxiv.org/abs/1905.01298},
year = {2019}
}
@article{Redmon2017,
abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster R-CNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time.},
archivePrefix = {arXiv},
arxivId = {1612.08242},
author = {Redmon, Joseph and Farhadi, Ali},
doi = {10.1109/CVPR.2017.690},
eprint = {1612.08242},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/YOLOv2.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {6517--6525},
title = {{YOLO9000: Better, faster, stronger}},
volume = {2017-Janua},
year = {2017}
}
@article{Chen2016,
abstract = {The goal of this paper is to perform 3D object detection from a single monocular image in the domain of autonomous driving. Our method first aims to generate a set of candidate class-specific object proposals, which are then run through a standard CNN pipeline to obtain highquality object detections. The focus of this paper is on proposal generation. In particular, we propose an energy minimization approach that places object candidates in 3D using the fact that objects should be on the ground-plane. We then score each candidate box projected to the image plane via several intuitive potentials encoding semantic segmentation, contextual information, size and location priors and typical object shape. Our experimental evaluation demonstrates that our object proposal generation approach significantly outperforms all monocular approaches, and achieves the best detection performance on the challenging KITTI benchmark, among published monocular competitors.},
author = {Chen, Xiaozhi and Kundu, Kaustav and Zhang, Ziyu and Ma, Huimin and Fidler, Sanja and Urtasun, Raquel},
doi = {10.1109/CVPR.2016.236},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/Monocular 3D Object Detection for Autonomous Driving.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {2147--2156},
title = {{Monocular 3D Object Detection for Autonomous Driving}},
volume = {2016-Decem},
year = {2016}
}
@article{Uijlings2013,
abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 {\%} recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/{\~{}}uijlings/SelectiveSearch. html). {\textcopyright} 2013 Springer Science+Business Media New York.},
author = {Uijlings, J. R.R. and {Van De Sande}, K. E.A. and Gevers, T. and Smeulders, A. W.M.},
doi = {10.1007/s11263-013-0620-5},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/SelectiveSearch.pdf:pdf},
issn = {09205691},
journal = {International Journal of Computer Vision},
number = {2},
pages = {154--171},
title = {{Selective search for object recognition}},
volume = {104},
year = {2013}
}
@article{Chabot2017,
abstract = {In this paper, we present a novel approach, called Deep MANTA (Deep Many-Tasks), for many-task vehicle analy- sis from a given image. A robust convolutional network is introduced for simultaneous vehicle detection, part local- ization, visibility characterization and 3D dimension esti- mation. Its architecture is based on a new coarse-to-fine object proposal that boosts the vehicle detection. Moreover, the Deep MANTA network is able to localize vehicle parts even if these parts are not visible. In the inference, the net- work's outputs are used by a real time robust pose estima- tion algorithm for fine orientation estimation and 3D vehi- cle localization. We show in experiments that our method outperforms monocular state-of-the-art approaches on ve- hicle detection, orientation and 3D location tasks on the very challenging KITTI benchmark. 1.},
archivePrefix = {arXiv},
arxivId = {arXiv:1703.07570v1},
author = {Chabot, Florian and Chaouch, Mohamed and Rabarisoa, Jaonary and Chateau, Thierry},
eprint = {arXiv:1703.07570v1},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/1703.07570.pdf:pdf},
title = {{Deep MANTA: A Coarse-to-fine Many-Task Network for joint 2D and 3D vehicle analysis from monocular image}},
year = {2017}
}
@article{Girshick2014,
abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30{\%} relative to the previous best result on VOC 2012 - achieving a mAP of 53.3{\%}. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/{\~{}}rbg/rcnn.},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/RCNN.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {580--587},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
year = {2014}
}
@misc{Rast2020,
title = {{Rasterization: a Practical Implementation (An Overview of the Rasterization Algorithm)}},
url = {https://www.scratchapixel.com/lessons/3d-basic-rendering/rasterization-practical-implementation/overview-rasterization-algorithm},
urldate = {2020-01-12}
}
@misc{massa2018mrcnn,
author = {Massa, Francisco and Girshick, Ross},
title = {{maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch}},
url = {https://github.com/facebookresearch/maskrcnn-benchmark},
urldate = {2020-04-06},
year = {2018}
}
@article{Yoo2015,
author = {Yoo, Donggeun and Park, Sunggyun and Lee, Joon-young and Paek, Anthony S},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/AttentionNet.pdf:pdf},
journal = {Iccv},
title = {{AttentionNet: Aggregating Weak Directions for Accurate Object Detection}}
}
@article{Mousavian2017,
abstract = {supplementary material for 3d deepbox},
author = {Mousavian, Arsalan and Anguelov, Dragomir and Flynn, John and Ko{\v{s}}eck{\'{a}}, Jana},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/3D-Deepbox-Supplementary.pdf:pdf},
journal = {Cvpr},
pages = {2},
title = {{3D bounding box material}},
year = {2017}
}
@article{Xiang2017,
abstract = {In Convolutional Neural Network (CNN)-based object detection methods, region proposal becomes a bottleneck when objects exhibit significant scale variation, occlusion or truncation. In addition, these methods mainly focus on 2D object detection and cannot estimate detailed properties of objects. In this paper, we propose subcategory-Aware CNNs for object detection. We introduce a novel region proposal network that uses subcategory information to guide the proposal generating process, and a new detection network for joint detection and subcategory classification. By using subcategories related to object pose, we achieve state of-The-Art performance on both detection and pose estimation on commonly used benchmarks.},
archivePrefix = {arXiv},
arxivId = {1604.04693},
author = {Xiang, Yu and Choi, Wongun and Lin, Yuanqing and Savarese, Silvio},
doi = {10.1109/WACV.2017.108},
eprint = {1604.04693},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/Subcategory-aware Convolutional Neural Networks for  SubCNN.pdf:pdf},
isbn = {9781509048229},
journal = {Proceedings - 2017 IEEE Winter Conference on Applications of Computer Vision, WACV 2017},
pages = {924--933},
title = {{Subcategory-Aware convolutional neural networks for object proposals {\&} detection}},
year = {2017}
}
@article{Hu2018,
abstract = {The most recent trend in estimating the 6D pose of rigid objects has been to train deep networks to either directly regress the pose from the image or to predict the 2D locations of 3D keypoints, from which the pose can be obtained using a PnP algorithm. In both cases, the object is treated as a global entity, and a single pose estimate is computed. As a consequence, the resulting techniques can be vulnerable to large occlusions. In this paper, we introduce a segmentation-driven 6D pose estimation framework where each visible part of the objects contributes a local pose prediction in the form of 2D keypoint locations. We then use a predicted measure of confidence to combine these pose candidates into a robust set of 3D-to-2D correspondences, from which a reliable pose estimate can be obtained. We outperform the state-of-the-art on the challenging Occluded-LINEMOD and YCB-Video datasets, which is evidence that our approach deals well with multiple poorly-textured objects occluding each other. Furthermore, it relies on a simple enough architecture to achieve real-time performance.},
archivePrefix = {arXiv},
arxivId = {1812.02541},
author = {Hu, Yinlin and Hugonot, Joachim and Fua, Pascal and Salzmann, Mathieu},
eprint = {1812.02541},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/Segmentation-driven 6D Object Pose Estimation.pdf:pdf},
title = {{Segmentation-driven 6D Object Pose Estimation}},
url = {http://arxiv.org/abs/1812.02541},
year = {2018}
}
@article{Fischler1981,
abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing},
author = {Fischler, Martin a and Bolles, Robert C},
doi = {10.1145/358669.358692},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/PnP {\&} RANSAC.pdf:pdf},
journal = {Communications of the ACM},
keywords = {0,1,2,3,5,60,61,71,8,analysis,and phrases,automated cartography,camera calibration,cr categories,determination,image matching,location,model fitting,scene},
number = {6},
pages = {381--395},
title = {{Paradigm for Model}},
volume = {24},
year = {1981}
}
@article{Mousavian2017a,
abstract = {We present a method for 3D object detection and pose estimation from a single image. In contrast to current techniques that only regress the 3D orientation of an object, our method first regresses relatively stable 3D object properties using a deep convolutional neural network and then combines these estimates with geometric constraints provided by a 2D object bounding box to produce a complete 3D bounding box. The first network output estimates the 3D object orientation using a novel hybrid discrete-continuous loss, which significantly outperforms the L2 loss. The second output regresses the 3D object dimensions, which have relatively little variance compared to alternatives and can often be predicted for many object types. These estimates, combined with the geometric constraints on translation imposed by the 2D bounding box, enable us to recover a stable and accurate 3D object pose. We evaluate our method on the challenging KITTI object detection benchmark [2] both on the official metric of 3D orientation estimation and also on the accuracy of the obtained 3D bounding boxes. Although conceptually simple, our method outperforms more complex and computationally expensive approaches that leverage semantic segmentation, instance level segmentation and flat ground priors [4] and sub-category detection [23][24]. Our discrete-continuous loss also produces state of the art results for 3D viewpoint estimation on the Pascal 3D+ dataset[26].},
archivePrefix = {arXiv},
arxivId = {1612.00496},
author = {Mousavian, Arsalan and Anguelov, Dragomir and Ko{\v{s}}eck{\'{a}}, Jana and Flynn, John},
doi = {10.1109/CVPR.2017.597},
eprint = {1612.00496},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/3D Bounding Box Estimation Using Deep Learning and Geometry.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {5632--5640},
title = {{3D bounding box estimation using deep learning and geometry}},
volume = {2017-Janua},
year = {2017}
}
@article{Simonelli2019a,
abstract = {In this paper we propose an approach for monocular 3D object detection from a single RGB image, which leverages a novel disentangling transformation for 2D and 3D detection losses and a novel, self-supervised confidence score for 3D bounding boxes. Our proposed loss disentanglement has the twofold advantage of simplifying the training dynamics in the presence of losses with complex interactions of parameters, and sidestepping the issue of balancing independent regression terms. Our solution overcomes these issues by isolating the contribution made by groups of parameters to a given loss, without changing its nature. We further apply loss disentanglement to another novel, signed Intersection-over-Union criterion-driven loss for improving 2D detection results. Besides our methodological innovations, we critically review the AP metric used in KITTI3D, which emerged as the most important dataset for comparing 3D detection results. We identify and resolve a flaw in the 11-point interpolated AP metric, affecting all previously published detection results and particularly biases the results of monocular 3D detection. We provide extensive experimental evaluations and ablation studies and set a new state-of-the-art on the KITTI3D Car class.},
archivePrefix = {arXiv},
arxivId = {1905.12365},
author = {Simonelli, Andrea and Bulo, Samuel Rota and Porzi, Lorenzo and Lopez-Antequera, Manuel and Kontschieder, Peter},
doi = {10.1109/ICCV.2019.00208},
eprint = {1905.12365},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/MonoDIS.pdf:pdf},
isbn = {9781728148038},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {1991--1999},
title = {{Disentangling monocular 3D object detection}},
volume = {2019-Octob},
year = {2019}
}
@article{Fidler2012,
abstract = {This paper addresses the problem of category-level 3D object detection. Given a monocular image, our aim is to localize the objects in 3D by enclosing them with tight oriented 3D bounding boxes. We propose a novel approach that extends the well-acclaimed deformable part-based model [1] to reason in 3D. Our model represents an object class as a deformable 3D cuboid composed of faces and parts, which are both allowed to deform with respect to their anchors on the 3D box. We model the appearance of each face in fronto-parallel coordinates, thus effectively factoring out the appearance variation induced by viewpoint. Our model reasons about face visibility patters called aspects. We train the cuboid model jointly and discriminatively and share weights across all aspects to attain efficiency. Inference then entails sliding and rotating the box in 3D and scoring object hypotheses. While for inference we discretize the search space, the variables are continuous in our model. We demonstrate the effectiveness of our approach in indoor and outdoor scenarios, and show that our approach significantly outperforms the state-of-the-art in both 2D [1] and 3D object detection [2].},
author = {Fidler, Sanja and Dickinson, Sven and Urtasun, Raquel},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {611--619},
title = {{3D object detection and viewpoint estimation with a deformable 3D cuboid model}},
volume = {1},
year = {2012}
}
@article{Pepik2015,
abstract = {Object class detection has been a synonym for 2D bounding box localization for the longest time, fueled by the success of powerful statistical learning techniques, combined with robust image representations. Only recently, there has been a growing interest in revisiting the promise of computer vision from the early days: to precisely delineate the contents of a visual scene, object by object, in 3D. In this paper, we draw from recent advances in object detection and 2D-3D object lifting in order to design an object class detector that is particularly tailored towards 3D object class detection. Our 3D object class detection method consists of several stages gradually enriching the object detection output with object viewpoint, keypoints and 3D shape estimates. Following careful design, in each stage it constantly improves the performance and achieves state-of-the-art performance in simultaneous 2D bounding box and viewpoint estimation on the challenging Pascal3D+ [50] dataset.},
archivePrefix = {arXiv},
arxivId = {1503.05038},
author = {Pepik, Bojan and Stark, Michael and Gehler, Peter and Ritschel, Tobias and Schiele, Bernt},
doi = {10.1109/CVPRW.2015.7301358},
eprint = {1503.05038},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/3D Object Class Detection in the Wild.pdf:pdf},
isbn = {9781467367592},
issn = {21607516},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
keywords = {Computational modeling,Design automation,Detectors,Pipelines,Shape,Solid modeling,Three-dimensional displays},
pages = {1--10},
title = {{3D object class detection in the wild}},
volume = {2015-Octob},
year = {2015}
}
@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
eprint = {1412.6980},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/ADAM.pdf:pdf},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
pages = {1--15},
title = {{Adam: A method for stochastic optimization}},
year = {2015}
}
@article{Crivellaro2018,
abstract = {We present an algorithm for estimating the pose of a rigid object in real-time under challenging conditions. Our method effectively handles poorly textured objects in cluttered, changing environments, even when their appearance is corrupted by large occlusions, and it relies on grayscale images to handle metallic environments on which depth cameras would fail. As a result, our method is suitable for practical Augmented Reality applications including industrial environments. At the core of our approach is a novel representation for the 3D pose of object parts: We predict the 3D pose of each part in the form of the 2D projections of a few control points. The advantages of this representation is three-fold: We can predict the 3D pose of the object even when only one part is visible; when several parts are visible, we can easily combine them to compute a better pose of the object; the 3D pose we obtain is usually very accurate, even when only few parts are visible. We show how to use this representation in a robust 3D tracking framework. In addition to extensive comparisons with the state-of-the-art, we demonstrate our method on a practical Augmented Reality application for maintenance assistance in the ATLAS particle detector at CERN.},
author = {Crivellaro, Alberto and Rad, Mahdi and Verdie, Yannick and Yi, Kwang Moo and Fua, Pascal and Lepetit, Vincent},
doi = {10.1109/TPAMI.2017.2708711},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/Robust 3D Object Tracking from Monocular Images Using Stable Parts.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {3D detection,3D tracking},
number = {6},
pages = {1465--1479},
publisher = {IEEE},
title = {{Robust 3D Object Tracking from Monocular Images Using Stable Parts}},
volume = {40},
year = {2018}
}
@article{Mottaghi2015,
abstract = {Despite the fact that object detection, 3D pose estimation, and sub-category recognition are highly correlated tasks, they are usually addressed independently from each other because of the huge space of parameters. To jointly model all of these tasks, we propose a coarse-to-fine hierarchical representation, where each level of the hierarchy represents objects at a different level of granularity. The hierarchical representation prevents performance loss, which is often caused by the increase in the number of parameters (as we consider more tasks to model), and the joint modeling enables resolving ambiguities that exist in independent modeling of these tasks. We augment PASCAL3D+ [34] dataset with annotations for these tasks and show that our hierarchical model is effective in joint modeling of object detection, 3D pose estimation, and sub-category recognition.},
archivePrefix = {arXiv},
arxivId = {1504.02764},
author = {Mottaghi, Roozbeh and Xiang, Yu and Savarese, Silvio},
doi = {10.1109/CVPR.2015.7298639},
eprint = {1504.02764},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/A Coarse-to-Fine Model for 3D Pose Estimation and Sub-category Recognition.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {418--426},
title = {{A coarse-to-fine model for 3D pose estimation and sub-category recognition}},
volume = {07-12-June},
year = {2015}
}
@article{Xia2017,
abstract = {While significant attention has been recently focused on designing supervised deep semantic segmentation algorithms for vision tasks, there are many domains in which sufficient supervised pixel-level labels are difficult to obtain. In this paper, we revisit the problem of purely unsupervised image segmentation and propose a novel deep architecture for this problem. We borrow recent ideas from supervised semantic segmentation methods, in particular by concatenating two fully convolutional networks together into an autoencoder--one for encoding and one for decoding. The encoding layer produces a k-way pixelwise prediction, and both the reconstruction error of the autoencoder as well as the normalized cut produced by the encoder are jointly minimized during training. When combined with suitable postprocessing involving conditional random field smoothing and hierarchical segmentation, our resulting algorithm achieves impressive results on the benchmark Berkeley Segmentation Data Set, outperforming a number of competing methods.},
archivePrefix = {arXiv},
arxivId = {1711.08506},
author = {Xia, Xide and Kulis, Brian},
eprint = {1711.08506},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/W-Net A Deep Model for Fully Unsupervised Image Segmentation.pdf:pdf},
title = {{W-Net: A Deep Model for Fully Unsupervised Image Segmentation}},
url = {http://arxiv.org/abs/1711.08506},
year = {2017}
}
@article{Shelhamer2016,
abstract = {Semantic image segmentation is a principal problem in computer vision, where the aim is to correctly classify each individual pixel of an image into a semantic label. Its widespread use in many areas, including medical imaging and autonomous driving, has fostered extensive research in recent years. Empirical improvements in tackling this task have primarily been motivated by successful exploitation of Convolutional Neural Networks (CNNs) pre-trained for image classification and object recognition. However, the pixel-wise labelling with CNNs has its own unique challenges: (1) an accurate deconvolution, or upsampling, of low-resolution output into a higher-resolution segmentation mask and (2) an inclusion of global information, or context, within locally extracted features. To address these issues, we propose a novel architecture to conduct the equivalent of the deconvolution operation globally and acquire dense predictions. We demonstrate that it leads to improved performance of state-of-the-art semantic segmentation models on the PASCAL VOC 2012 benchmark, reaching 74.0{\%} mean IU accuracy on the test set.},
archivePrefix = {arXiv},
arxivId = {1605.06211},
author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
doi = {10.5244/C.30.124},
eprint = {1605.06211},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/FCN.pdf:pdf},
journal = {British Machine Vision Conference 2016, BMVC 2016},
pages = {124.1--124.14},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
volume = {2016-Septe},
year = {2016}
}
@article{Liu2016,
abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300×300 input, SSD achieves 74.3{\%} mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512 × 512 input, SSD achieves 76.9{\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/ tree/ssd.},
archivePrefix = {arXiv},
arxivId = {arXiv:1512.02325v5},
author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng Yang and Berg, Alexander C.},
doi = {10.1007/978-3-319-46448-0_2},
eprint = {arXiv:1512.02325v5},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/SSD.pdf:pdf},
isbn = {9783319464473},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Convolutional neural network,Real-time object detection},
pages = {21--37},
title = {{SSD: Single shot multibox detector}},
volume = {9905 LNCS},
year = {2016}
}
@article{Sahin2020,
abstract = {Object pose recovery has gained increasing attention in the computer vision field as it has become an important problem in rapidly evolving technological areas related to autonomous driving, robotics, and augmented reality. Existing review-related studies have addressed the problem at visual level in 2D, going through the methods which produce 2D bounding boxes of objects of interest in RGB images. The 2D search space is enlarged either using the geometry information available in the 3D space along with RGB (Mono/Stereo) images, or utilizing depth data from LIDAR sensors and/or RGB-D cameras. 3D bounding box detectors, producing category-level amodal 3D bounding boxes, are evaluated on gravity aligned images, while full 6D object pose estimators are mostly tested at instance-level on the images where the alignment constraint is removed. Recently, 6D object pose estimation is tackled at the level of categories. In this paper, we present the first comprehensive and most recent review of the methods on object pose recovery, from 3D bounding box detectors to full 6D pose estimators. The methods mathematically model the problem as a classification, regression, classification {\&} regression, template matching, and point-pair feature matching task. Based on this, a mathematical-model-based categorization of the methods is established. Datasets used for evaluating the methods are investigated with respect to the challenges, and evaluation metrics are studied. Quantitative results of experiments in the literature are analyzed to show which category of methods best performs across what types of challenges. The analyses are further extended comparing two methods, which are our own implementations, so that the outcomes from the public results are further solidified. Current position of the field is summarized regarding object pose recovery, and possible research directions are identified.},
archivePrefix = {arXiv},
arxivId = {arXiv:2001.10609v2},
author = {Sahin, Caner and Garcia-Hernando, Guillermo and Sock, Juil and Kim, Tae Kyun},
doi = {10.1016/j.imavis.2020.103898},
eprint = {arXiv:2001.10609v2},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/overwiew.pdf:pdf},
issn = {02628856},
journal = {Image and Vision Computing},
pages = {1--25},
title = {{A review on object pose recovery: From 3D bounding box detectors to full 6D pose estimators}},
volume = {96},
year = {2020}
}
@article{Shen2017,
abstract = {We present Deeply Supervised Object Detector (DSOD), a framework that can learn object detectors from scratch. State-of-the-art object objectors rely heavily on the off the-shelf networks pre-trained on large-scale classification datasets like Image Net, which incurs learning bias due to the difference on both the loss functions and the category distributions between classification and detection tasks. Model fine-tuning for the detection task could alleviate this bias to some extent but not fundamentally. Besides, transferring pre-trained models from classification to detection between discrepant domains is even more difficult (e.g. RGB to depth images). A better solution to tackle these two critical problems is to train object detectors from scratch, which motivates our proposed DSOD. Previous efforts in this direction mostly failed due to much more complicated loss functions and limited training data in object detection. In DSOD, we contribute a set of design principles for training object detectors from scratch. One of the key findings is that deep supervision, enabled by dense layer-wise connections, plays a critical role in learning a good detector. Combining with several other principles, we develop DSOD following the single-shot detection (SSD) framework. Experiments on PASCAL VOC 2007, 2012 and MS COCO datasets demonstrate that DSOD can achieve better results than the state-of-the-art solutions with much more compact models. For instance, DSOD outperforms SSD on all three benchmarks with real-time detection speed, while requires only 1/2 parameters to SSD and 1/10 parameters to Faster RCNN.},
archivePrefix = {arXiv},
arxivId = {1708.01241},
author = {Shen, Zhiqiang and Liu, Zhuang and Li, Jianguo and Jiang, Yu Gang and Chen, Yurong and Xue, Xiangyang},
doi = {10.1109/ICCV.2017.212},
eprint = {1708.01241},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/DSOD.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {1937--1945},
title = {{DSOD: Learning Deeply Supervised Object Detectors from Scratch}},
volume = {2017-Octob},
year = {2017}
}
@article{Ji2018,
abstract = {We present a novel clustering objective that learns a neural network classifier from scratch, given only unlabelled data samples. The model discovers clusters that accurately match semantic classes, achieving state-of-the-art results in eight unsupervised clustering benchmarks spanning image classification and segmentation. These include STL10, an unsupervised variant of ImageNet, and CIFAR10, where we significantly beat the accuracy of our closest competitors by 6.6 and 9.5 absolute percentage points respectively. The method is not specialised to computer vision and operates on any paired dataset samples; in our experiments we use random transforms to obtain a pair from each image. The trained network directly outputs semantic labels, rather than high dimensional representations that need external processing to be usable for semantic clustering. The objective is simply to maximise mutual information between the class assignments of each pair. It is easy to implement and rigorously grounded in information theory, meaning we effortlessly avoid degenerate solutions that other clustering methods are susceptible to. In addition to the fully unsupervised mode, we also test two semi-supervised settings. The first achieves 88.8{\%} accuracy on STL10 classification, setting a new global state-of-the-art over all existing methods (whether supervised, semi-supervised or unsupervised). The second shows robustness to 90{\%} reductions in label coverage, of relevance to applications that wish to make use of small amounts of labels. github.com/xu-ji/IIC},
archivePrefix = {arXiv},
arxivId = {1807.06653},
author = {Ji, Xu and Henriques, Jo{\~{a}}o F. and Vedaldi, Andrea},
eprint = {1807.06653},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/Ji{\_}Invariant{\_}Information{\_}Clustering{\_}for{\_}Unsupervised{\_}Image{\_}Classification{\_}and{\_}Segmentation{\_}ICCV{\_}2019{\_}paper (1).pdf:pdf},
number = {Iic},
title = {{Invariant Information Clustering for Unsupervised Image Classification and Segmentation}},
url = {http://arxiv.org/abs/1807.06653},
year = {2018}
}
@misc{Stanford2019,
title = {{Stanford University CS231n: Convolutional Neural Networks for Visual Recognition}},
url = {http://cs231n.stanford.edu/2019/},
urldate = {2020-04-01}
}
@article{Ferrari2006,
abstract = {We present a novel Object Recognition approach based on affine invariant regions. It actively counters the problems related to the limited repeatability of the region detectors, and the difficulty of matching, in the presence of large amounts of background clutter and particularly challenging viewing conditions. After producing an initial set of matches, the method gradually explores the surrounding image areas, recursively constructing more and more matching regions, increasingly farther from the initial ones. This process covers the object with matches, and simultaneously separates the correct matches from the wrong ones. Hence, recognition and segmentation are achieved at the same time. The approach includes a mechanism for capturing the relationships between multiple model views and exploiting these for integrating the contributions of the views at recognition time. This is based on an efficient algorithm for partitioning a set of region matches into groups lying on smooth surfaces. Integration is achieved by measuring the consistency of configurations of groups arising from different model views. Experimental results demonstrate the stronger power of the approach in dealing with extensive clutter, dominant occlusion, and large scale and viewpoint changes. Non-rigid deformations are explicitly taken into account, and the approximative contours of the object are produced. All presented techniques can extend any view-point invariant feature extractor. {\textcopyright} 2006 Springer Science + Business Media, Inc.},
author = {Ferrari, Vittorio and Tuytelaars, Tinne and {Van Gool}, Luc},
doi = {10.1007/s11263-005-3964-7},
file = {:C$\backslash$:/Users/Teo/Documents/Engineering/Year4/4YP/Papers/Simultaneous{\_}Object{\_}Recognition{\_}and{\_}Segmentation{\_}f.pdf:pdf},
issn = {09205691},
journal = {International Journal of Computer Vision},
number = {2},
pages = {159--188},
title = {{Simultaneous object recognition and segmentation from single or multiple model views}},
volume = {67},
year = {2006}
}
