\documentclass[main.tex]{subfiles}

\begin{document}

\section{Literature Review}
\subsection{Sensors}
Computer vision is enabled by sensors that capture the state of the surrounding world at a single instance and transform it into machine readable data. Although a variety of signals can be used to make inferences about the environment, electromagnetic radiation is by far the most informative one. Therefore, various devices that detect different parts of the light spectra have been utilised in machine vision. \\
\indent Modelled after the human vision system, the traditional camera has quickly become the most widespread computer vision sensor. It responds to light with wavelengths in the visible spectra and similarly to the human eye, differentiates between three main colours - red, green and blue (RGB). Visible-light cameras are often used as standalone monocular sensors or in stereo or multi-sensor configurations which fuse information from different viewpoints and provide depth perception.\\
\indent Another popular device is the RGB-D camera. In this case, the produced three channel RGB image is accompanied by a depth map. This is achieved by adding an infra-red (IR) emitter that detects its own light reflected from the environment. Depth is then estimated from the return time of the light beam. The detection range of RGB-D cameras is usually a few metres. \\
\indent The LIDAR - a pure time-of-flight laser sensor that operates in the optical and infra-red spectra, is frequently used in autonomous vehicles and robots for mapping and visualisation purposes. It provides relatively sparse but very accurate distance measurements in the form of 3D point clouds. A LIDAR's detection range can be up to a few hundred metres.\\
\indent Similar devices working with radio and sound waves respectively are the RADAR and SONAR. They are usually implemented for more specialised applications such as aircraft or marine but the resulting data can often be processed by standard computer vision methods. The specific propagation of sound and radio waves allows these devices to perform at distances of a few hundred kilometres. \\
\indent Yet another sensor working in the infra-red spectrum is the thermal camera. Instead of detecting its own light, however, it measures the heat signature coming from the environment which is useful in situations where large heat gradients exist.\\
\indent Other devices that utilize electromagnetic radiation with shorter wavelengths such as UV light and X-rays have been of crucial importance for the medical and security sectors. \\
\indent Despite the abundance of devices that can produce visually meaningful information, many of them are expensive and need complex maintenance. Consequently, traditional cameras are preferred in most applications for their affordability and mobility. The widespread use of smart-phones which are equipped with at least one camera allows the developed algorithms to be used on mobile devices as well. In addition, the use of a single sensor reduces both the cost and the amount of required input data to minimum. This has made scientific research in computer vision much more approachable for people of different backgrounds. \\
\indent Single camera systems in computer vision are justified by their functional similarity to the human visual system. By utilizing accumulated knowledge in the brain, people are capable of understanding the contents of a 2D image and infer their relationships without relying on 3D information. Thus monocular methods for computer vision combined with prior knowledge about the observed environment can be used to emulate human vision. As a result, a lot of research in the area has been done in recent years.

\subsection{2D Object Detection}
\indent In some cases, making inferences about the three dimensional characteristics of objects requires accurate 2D object detection. In the classical sense, 2D object detection aims to localise and classify available objects in a scene by assigning a rectangular 2D bounding box and category affiliation for each of them. In general, there exist two types of supervised approaches that deal with this problem \cite{Zhao2019}. The first one formulates it as a regression/classification task and attempts to directly translate image pixels into bounding box coordinates and classification probabilities. Some notable examples are MultiBox \cite{Erhan2014}, AttentionNet \cite{Yoo2015}, G-CNN \cite{Najibi2016}, YOLO \cite{Redmon2016}, SSD \cite{Liu2016}, YOLOv2 \cite{Redmon2017}, DSSD \cite{Fu2017} and DSOD \cite{Shen2017}. This project, however, will limit itself to the second approach which relies on region proposals to reason about object positions and classify them. \\
\indent An early method uses the selective search algorithm \cite{Uijlings2013} to generate a set of region proposals which are then fed into a convolutional neural network (CNN) for feature extraction and classification (R-CNN \cite{Girshick2014}). Consequent modifications improved the performance by extracting region proposals from the features output from the CNN instead of the image itself (Fast R-CNN s\cite{Girshick2015}). Eventually, the slow and computationally heavy algorithms that generated the region proposals were replaced by a region proposal network (RPN) whose outputs are refined and classified in the rest of the model (Faster R-CNN \cite{Ren2017}). An improvement in the layer used to extract proposed regions from the feature maps resulted in further increase in performance accuracy (Mask R-CNN \cite{He2017}). \\
\indent An exhaustive account of available approaches is provided by Zhao et al. \cite{Zhao2019}. Current state of the art methods on different datasets rely on some of the aforementioned structures but perform significant optimisations that reduce training time and required number of parameters. Although far from perfect, they provide sufficiently good results to enable other more complex tasks.
\subsection{3D Object Detection}
\indent 3D object detection aims to determine the real position, dimensions and orientation of objects in a scene. It can be expressed as either of two interrelated tasks - 3D bounding box detection or 6DOF pose estimation. The expectation to recover 3D information from 2D images makes it one of the hardest problems in computer vision. This is due to the fact that the essential depth dimension is collapsed and thus lost during the process of image formation. The latter results in a significant step up in the number of degrees of freedom. Therefore a vast variety of approaches to this problem exist.\\
\indent Naturally, 3D information about an object can be regressed directly through heavy utilization of neural network frameworks which map image pixels to 3D bounding box coordinates and orientation. Other methods take a stepwise approach and build up to the desired output. They often make use of prior knowledge about the environment, introduce various constraints, rely on specifically designed constructs and establish correspondences between 2D and 3D features. An exhaustive overview of available pose estimation methods is provided by Sahin et al \cite{Sahin2020}. Due to the large amount of existing literature, the focus will be on monocular methodologies directly relevant to the current project.
\subsection{Part-Based Methods}
\indent Some of the earliest methods for pose estimation were based on detection of various object parts and their relative positions. The use of parts is scientifically justified by other fields such as biology and psychology which claim that human vision recognises objects and their orientations by combining information about their building blocks \cite{Biederman1987}. Naturally, this led to the development of deformable part models (DPMs)\cite{Felzenszwalb2009}. They represent objects as combinations of parts and introduce constraints between them to preserve their underlying geometrical arrangement.\\
\indent Initially designed to work in 2D, DPMs were later successfully adapted to reason in 3D. In order to do so, a discriminative DPM has been extended to predict viewpoints and detect three dimensional object parts. Similar to the 2D version, object parts are constrained by corresponding 3D conditions. Consequently, the continuous object pose can be recovered successfully at the expense of some 2D detection accuracy \cite{Pepik2012}.\\
\indent The concept of DPMs operating in 3D has since been extended further through the definition of a deformable 3D cuboid model. It consists of faces which are made up of parts and are constrained by 'stitching points'. Both constructs are allowed to deviate from their anchor positions. A cuboid template is learned for each object category. During inference features extracted from images and different viewpoint projections of the template are compared to determine the best matching object pose \cite{Fidler2012}.\\
\indent More recent methods attempt to estimate the 3D pose of an object by combining the 3D poses of a discrete number of its parts. For this purpose a few specific and easily distinguishable parts are detected and fed into a CNN which predicts the 2D coordinates of projected 3D control points. The assumption of a rough pose prior allows the final 3D pose to be determined by solving an optimization problem over the identified object parts \cite{Crivellaro2018}. \\ 
\indent The results achieved by these works confirm the theoretical assumption that parts and their relative spatial positions carry significant information about the object as a whole. Thus focusing a network's attention to the structural elements of an object of interest can increase the model's performance. In addition, there exists relatively little contemporary literature on part-based monocular methods which identifies them as potentially interesting research area.
\subsection{Regression Methods}
\indent Over the past few years there has been a notable improvement in the performance of various CNN frameworks and deep learning models. Consequently, an increasing number of methods opt to regress 3D object dimensions and viewpoint parameters directly. This often results in one-stage networks which combined with some specific domain constraints attempt to discover some mapping between the available input information and the desired output.\\
\indent One approach augments a RPN to predict 3D bounding box coordinates, orientation and real world dimensions. It implements a hybrid-multi-bin loss to classify the orientation in one of $n$ discrete angles. Small viewpoint corrections and deviations from the mean vehicle dimensions are regressed directly. Finally, the resulting 3D bounding box is constrained to fit in the 2D detection window to determine the pose \cite{Mousavian2017a}.\\
\indent Another interesting work uses ground plane and location priors to propose 3D bounding boxes. The proposals are then projected down to the 2D image for evaluation. Semantic and instance segmentation ensure that an object of the correct class is present in each region. Shape and context losses make sure that the bounding box fits well and is located above a 'road'. The proposals are evaluated with Fast R-CNN to determine the best 3D bounding box.\\
\indent Numerous recent works such as MonoDIS \cite{Simonelli2019a}, SS3D \cite{Jorgensen2019} and M3D-RPN \cite{Brazil2019} regress 3D parameters directly. Such methods rely predominantly on the modelling power inherent in neural networks and combine it with appropriate loss functions and constraints.
\subsection{CAD Model Methods}
\indent The camera maps 3D points onto a 2D image plane and thus removes essential spatial information. The use of prior knowledge about the 3D shape of available objects can reduce the degrees of freedom and simplify the task. Therefore many methods use CAD models to aid the pose recovery process.\\
\indent A common way to incorporate CAD models is to align them to corresponding 2D object instances. One approach uses modified R-CNNs to detect and classify objects, predict class-specific 2D key-point locations and estimate a continuous azimuth angle. A spatial model relates the bounding box centre with the key-point positions. In the end, the viewpoint is estimated through minimisation of the re-projection error \cite{Pepik2015}.\\
\indent In other works CAD models are used as a template for comparison with detected 2D object instances. An approach that follows this strategy addresses the problem in different steps at various levels of detail from coarse to fine. The reason for this is that fine sub-category classification is closely correlated with object pose. A hierarchical model represents each different level as a combination of discrete and continuous random variables of the object labels and viewpoints. Shallow CNNs encode features from 2D object detections and CAD models rendered in a particular viewpoint. These are then compared to determine the pose \cite{Mottaghi2015}.\\
\indent Alternative methods incorporate 3D models within their own special constructs to handle pose estimation. One of these is the so called 3D voxel pattern (3DVP) which contains information about object appearance, 3D shape, occlusions and truncations. CAD models are aligned with a given object instance to create voxel models with encoded visibility information. A CNN is then trained to recognise these 3D voxel exemplars which provide 2D image, 2D segmentation mask and the 3D voxel model for each instance \cite{Xiang2015}. \\
\indent The aforementioned approaches prove that the existence of CAD models or other prior knowledge about the object shape can make the ill-posed 3D object detection problem tractable.
\subsection{Key-Point Methods}
\indent In the majority of cases when the viewpoint is not regressed directly, some key-point correspondences between 2D and 3D are used to estimate the transformation from one domain to the other. The so occurring problem is widely known as the perspective n-point problem (PnP) and has been first introduced by M. A. Fischler and C. R. Bolles in 1981 \cite{Fischler1981}. \\
\indent The PnP problem can be solved by minimising a given error function or in some cases - analytically. Often many of the available data points are inaccurate or corrupted by noise which can result in skewed estimation of the transformation parameters. Therefore the same authors propose the use of a random sample consensus (RANSAC \cite{Fischler1981}). It allows outliers to be detected and reduces their effect. Over time many improvements in efficiency, speed and accuracy have been made on both the PnP solution and the RANSAC algorithm but the general idea has remained the same.\\
\indent Some of the first works to incorporate this idea extend 2D object detection frameworks to predict 2D key-point locations and a 3D bounding box. The bounding box is used to find the best matching CAD model and the 2D-3D key-point correspondence problem is solved with EPnP \cite{Moreno-noguer2007} to estimate the object pose \cite{Chabot2017}.\\
\indent In the case when key-point annotated 2D images and object segmentations are available as ground truth, it is possible to produce 3D oriented shape models of detected objects. During training NRSfM \cite{Bregler2000} estimates viewpoints for each instance and constructs class-specific shape models. During inference, the object detections are segmented and a CNN predicts viewpoint and sub-category. Finally, the model is rotated to fit the viewpoint resulting in a 3D oriented shape for each object instance \cite{Kar2015}.\\
\indent The closest work to the method proposed in this project makes use of object segmentation to tackle occlusions. It argues that a holistic approach to object detection is inefficient and attention should be paid to each visible part separately. The segmentation approach allows to perform object detection and pose estimation simultaneously. Two CNN streams are implemented to segment the images and predict projections of 3D bounding box corners. Each pixel that belongs to the object then votes for the positions of each of the key-points and RANSAC is used to get a final pose estimate. Nevertheless, the method finds it difficult to handle tiny objects and extreme occlusions \cite{Hu2018}.
\end{document}